#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Fri Mar  2 08:37:39 2018

@author: clararebbelstam
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis
from sklearn.metrics import confusion_matrix, classification_report, precision_score

#Import Smarket data: 
df = pd.read_csv('Smarket.csv', usecols=range(1,10), index_col=0, parse_dates=True)
df.head()

#Splits data into training set and test set. 
#Data from 2004 and before are used as training set, and data from 2005 are used as test set. 
X_train = df[:'2004'][['Lag1','Lag2']]
y_train = df[:'2004']['Direction']

X_test = df['2005':][['Lag1','Lag2']]
y_test = df['2005':]['Direction']

#Performs linear discriminant analysis on the training set 
lda = LinearDiscriminantAnalysis()
model = lda.fit(X_train, y_train)

#Prior probabilities of groups: 
print(model.priors_)

#Group means: 
print(model.means_)

#Coefficients of linear discriminants:
print(model.coef_)

#Performs predictions on the test set: 
pred=model.predict(X_test)
print(np.unique(pred, return_counts=True))

#Confusion matrix for predictions: 
print(confusion_matrix(pred, y_test))

#Classification details: 
print(classification_report(y_test, pred, digits=3))

#Predicted probabilities: 
pred_p = model.predict_proba(X_test)

#Applying 50% threshold to posterior probabilities allows us 
#to recreate predictions: 
print(np.unique(pred_p[:,1]>0.5, return_counts=True))

#Applying 90% threshold: 
print(np.unique(pred_p[:,1]>0.9, return_counts=True))

#Findind maximum posterior probability: 
max(pred_p[:,1])
#%% 4.6.4 Quadratic Discriminant Analysis
# We will now fit a QDA model to the `Smarket` data. QDA is implemented
# in `sklearn` using the `QuadraticDiscriminantAnalysis()` function, which is again part of the `discriminant_analysis` module. The
# syntax is identical to that of `LinearDiscriminantAnalysis()`.

qda = QuadraticDiscriminantAnalysis()
model2 = qda.fit(X_train, y_train)
print(model2.priors_)
print(model2.means_)


# The output contains the group means. But it does not contain the coefficients
# of the linear discriminants, because the QDA classifier involves a
# _quadratic_, rather than a linear, function of the predictors. The `predict()`
# function works in exactly the same fashion as for LDA.

# In[14]:

pred2=model2.predict(X_test)
print(np.unique(pred2, return_counts=True))
print(confusion_matrix(pred2, y_test))
print(classification_report(y_test, pred2, digits=3))


# Interestingly, the QDA predictions are accurate almost 60% of the time,
# even though the 2005 data was not used to fit the model. This level of accuracy
# is quite impressive for stock market data, which is known to be quite
# hard to model accurately. 
# 
# This suggests that the quadratic form assumed
# by QDA may capture the true relationship more accurately than the linear
# forms assumed by LDA and logistic regression. However, we recommend
# evaluating this methodâ€™s performance on a larger test set before betting
# that this approach will consistently beat the market!
# 
# # An Application to Carseats Data
# Let's see how the `LDA/QDA` approach performs on the `Carseats` data set, which is
# included with `ISLR`. 
# 
# Recall: this is a simulated data set containing sales of child car seats at 400 different stores.

# In[15]:

df2 = pd.read_csv('Carseats.csv')
df2.head()


# See if you can build a model that predicts `ShelveLoc`, the shelf location (Bad, Good, or Medium) of the product at each store. Don't forget to hold out some of the data for testing!

# In[ ]:

# Your code here


# To get credit for this lab, please post your answers to the prompt in [#lab5](https://sds293.slack.com/messages/C7CR96LJ3).
